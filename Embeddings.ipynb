{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6bab53e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import InferenceClient\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http import models\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44467277",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "client = InferenceClient(\n",
    "    provider=\"hf-inference\",\n",
    "    api_key=os.environ[\"HF_TOKEN\"],\n",
    ")\n",
    "\n",
    "database = QdrantClient(\n",
    "    url=os.environ[\"QDRANT_URL\"],\n",
    "    api_key=os.environ[\"QDRANT_API_KEY\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8577615",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\OSHITH\\AppData\\Local\\Temp\\ipykernel_5748\\914487613.py:1: DeprecationWarning: `recreate_collection` method is deprecated and will be removed in the future. Use `collection_exists` to check collection existence and `create_collection` instead.\n",
      "  database.recreate_collection(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "database.recreate_collection(\n",
    "    collection_name=\"PassportKnowledgeBase\",\n",
    "    vectors_config={\n",
    "        \"chunk_vector\": models.VectorParams(size=768, distance=models.Distance.COSINE),\n",
    "        \"summary_vector\": models.VectorParams(size=768, distance=models.Distance.COSINE)\n",
    "    },\n",
    "    on_disk_payload=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c366ad83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    text = re.sub(r'#{1,6}\\s*', '', text)\n",
    "    text = re.sub(r'(\\*\\*|__)(.*?)\\1', r'\\2', text)\n",
    "    text = re.sub(r'(\\*|_)(.*?)\\1', r'\\2', text)\n",
    "    text = re.sub(r'\\|', ' ', text)\n",
    "    text = re.sub(r'-{3,}', '', text)\n",
    "    text = re.sub(r'={3,}', '', text)\n",
    "    text = re.sub(r'[\\*=\\-_]{3,}', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = text.replace('\\\\n', ' ').replace('\\\\t', ' ')\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "with open('chunkedKnowledge.txt', 'r', encoding='utf-8') as f:\n",
    "    content = f.read()\n",
    "    chunks = [chunk.strip() for chunk in content.split('\\n\\n') if chunk.strip()]\n",
    "\n",
    "chunk_embeddings = []\n",
    "summary_embeddings = []\n",
    "texts = []\n",
    "summaries = []\n",
    "\n",
    "for i, chunk in enumerate(chunks):\n",
    "    try:\n",
    "        cleanedText = clean_text(chunk)\n",
    "        \n",
    "        chunk_vector = client.feature_extraction(\n",
    "            text=cleanedText,\n",
    "            model=\"google/embeddinggemma-300m\"\n",
    "        )\n",
    "        summary_result = client.summarization(\n",
    "            text=cleanedText,\n",
    "            model=\"facebook/bart-large-cnn\"\n",
    "        )\n",
    "        \n",
    "        summary_text = summary_result.summary_text\n",
    "        \n",
    "        summary_vector = client.feature_extraction(\n",
    "            text=summary_text,\n",
    "            model=\"google/embeddinggemma-300m\"\n",
    "        )\n",
    "        \n",
    "        chunk_embeddings.append(chunk_vector)\n",
    "        summary_embeddings.append(summary_vector)\n",
    "        texts.append(cleanedText)\n",
    "        summaries.append(summary_text)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing chunk {i + 1}: {e}\")\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "89f8844a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, chunk in enumerate(chunks):\n",
    "    database.upsert(\n",
    "        collection_name=\"PassportKnowledgeBase\",\n",
    "        points=[\n",
    "            models.PointStruct(\n",
    "                id=i,\n",
    "                vector={\n",
    "                    \"chunk_vector\": chunk_embeddings[i],\n",
    "                    \"summary_vector\": summary_embeddings[i]\n",
    "                },\n",
    "                payload={\n",
    "                    \"text\": texts[i],\n",
    "                    \"summary\": summaries[i]\n",
    "                }\n",
    "            )\n",
    "        ]\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
